{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NN Template \u00b6 \"We demand rigidly defined areas of doubt and uncertainty.\" cookiecutter https://github.com/grok-ai/nn-template Generic cookiecutter template to bootstrap PyTorch projects and to avoid writing boilerplate code to integrate: PyTorch Lightning , lightweight PyTorch wrapper for high-performance AI research. Hydra , a framework for elegantly configuring complex applications. Weights and Biases , organize and analyze machine learning experiments. (educational account available) Streamlit , turns data scripts into shareable web apps in minutes. MkDocs and Material for MkDocs , a fast, simple and downright gorgeous static site generator. DVC , track large files, directories, or ML models. Think \"Git for data\". GitHub Actions , to run the tests, publish the documentation and to PyPI automatically. Python best practices for developing and publishing research projects. cookiecutter This is a parametrized template that uses cookiecutter . Install cookiecutter with: pip install cookiecutter","title":"Home"},{"location":"#nn-template","text":"\"We demand rigidly defined areas of doubt and uncertainty.\" cookiecutter https://github.com/grok-ai/nn-template Generic cookiecutter template to bootstrap PyTorch projects and to avoid writing boilerplate code to integrate: PyTorch Lightning , lightweight PyTorch wrapper for high-performance AI research. Hydra , a framework for elegantly configuring complex applications. Weights and Biases , organize and analyze machine learning experiments. (educational account available) Streamlit , turns data scripts into shareable web apps in minutes. MkDocs and Material for MkDocs , a fast, simple and downright gorgeous static site generator. DVC , track large files, directories, or ML models. Think \"Git for data\". GitHub Actions , to run the tests, publish the documentation and to PyPI automatically. Python best practices for developing and publishing research projects. cookiecutter This is a parametrized template that uses cookiecutter . Install cookiecutter with: pip install cookiecutter","title":"NN Template"},{"location":"papers/","text":"Scientific Papers based on nn-template \u00b6 The following papers acknowledge the adoption of NN Template: Findings of the Association for Computational Linguistics: EMNLP 2021 WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER Simone Tedeschi, Valentino Maiorca, Niccol\u00f2 Campolungo, Francesco Cecconi, and Roberto Navigli Findings of the Association for Computational Linguistics: EMNLP 2021 Named Entity Recognition for Entity Linking: What Works and What's Next. Simone Tedeschi, Simone Conia, Francesco Cecconi, and Roberto Navigli Please let us know if your paper also does and we'll add it to the list!","title":"Publications"},{"location":"papers/#scientific-papers-based-on-nn-template","text":"The following papers acknowledge the adoption of NN Template: Findings of the Association for Computational Linguistics: EMNLP 2021 WikiNEuRal: Combined Neural and Knowledge-based Silver Data Creation for Multilingual NER Simone Tedeschi, Valentino Maiorca, Niccol\u00f2 Campolungo, Francesco Cecconi, and Roberto Navigli Findings of the Association for Computational Linguistics: EMNLP 2021 Named Entity Recognition for Entity Linking: What Works and What's Next. Simone Tedeschi, Simone Conia, Francesco Cecconi, and Roberto Navigli Please let us know if your paper also does and we'll add it to the list!","title":"Scientific Papers based on nn-template"},{"location":"changelog/","text":"Changelog \u00b6 See the changelog in the releases page.","title":"Changelog"},{"location":"changelog/#changelog","text":"See the changelog in the releases page.","title":"Changelog"},{"location":"changelog/upgrade/","text":"The need for upgrading the template itself is lessened thanks to the nn-template-core library decoupling. Info Update the nn-template-core library changing the version constraint in the setup.cfg . However, you can use cruft to automate also the template updates!","title":"Upgrade"},{"location":"features/bestpractices/","text":"Tooling \u00b6 The template configures are the tooling necessary for a modern Python project. These include: EditorConfig maintain consistent coding styles for multiple developers. Black the uncompromising code formatter. isort sort imports alphabetically, and automatically separated into sections and by type. flake8 check coding style (PEP8), programming errors and cyclomatic complexity. pydocstyle static analysis tool for checking compliance with Python docstring conventions. MyPy static type checker for Python. Coverage measure code coverage of Python programs. bandit security linter from PyCQA. pre-commit framework for managing and maintaining pre-commit hooks. Pre commits \u00b6 The pre-commits configuration is defined in .pre-commit-config.yaml , and includes the most important checks and auto-fix to perform. If one of the pre-commits fails, the commit is aborted avoiding distraction errors. Info The pre-commits are also run in the CI/CD as part of the Test Suite. This helps guaranteeing the all the contributors are respecting the code conventions in the pull requests.","title":"Tooling"},{"location":"features/bestpractices/#tooling","text":"The template configures are the tooling necessary for a modern Python project. These include: EditorConfig maintain consistent coding styles for multiple developers. Black the uncompromising code formatter. isort sort imports alphabetically, and automatically separated into sections and by type. flake8 check coding style (PEP8), programming errors and cyclomatic complexity. pydocstyle static analysis tool for checking compliance with Python docstring conventions. MyPy static type checker for Python. Coverage measure code coverage of Python programs. bandit security linter from PyCQA. pre-commit framework for managing and maintaining pre-commit hooks.","title":"Tooling"},{"location":"features/bestpractices/#pre-commits","text":"The pre-commits configuration is defined in .pre-commit-config.yaml , and includes the most important checks and auto-fix to perform. If one of the pre-commits fails, the commit is aborted avoiding distraction errors. Info The pre-commits are also run in the CI/CD as part of the Test Suite. This helps guaranteeing the all the contributors are respecting the code conventions in the pull requests.","title":"Pre commits"},{"location":"features/cicd/","text":"CI/CD \u00b6 The generated project contains two GiHub Actions workflow to run the Test Suite and to publish you project. Note You need to enable the GitHub Actions from the settings in your repository. Important All the workflow already implement the logic needed to cache the conda and pip environment between workflow runs. Warning The annotated tags in the git repository to manage releases should follow the semantic versioning conventions: <major>.<minor>.<patch> Test Suite \u00b6 The Test Suite runs automatically for each commit in a Pull Request. It is successful if: The pre-commits do not raise any errors All the tests pass After that, the PR are marked with \u2714\ufe0f or \u274c depending on the test suite results. Publish docs \u00b6 The first time you should use mike to: create the gh-pages branch and specify the default docs version. mike deploy 0 .1 latest --push mike set-default latest Warning You do not need to execute these commands if you accepted the optional cookiecutter setup step. Info Remember to enable the GitHub Pages from the repository settings. After that, the docs are built and automatically published on release on GitHub Pages. This means that every time you publish a new release in your project an associated version of the documentation is published. Important The documentation version utilizes only the <major>.<minor> version of the release tag, discarding the patch version. Publish PyPi \u00b6 To publish your package on PyPi it is enough to configure the PyPi token in the GitHub repository secrets and de-comment the following in the publish.yaml workflow: - name : Build SDist and wheel run : pipx run build - name : Check metadata run : pipx run twine check dist/* - name : Publish distribution \ud83d\udce6 to PyPI uses : pypa/gh-action-pypi-publish@release/v1 with : user : __token__ password : ${{ secrets.PYPI_API_TOKEN }} In this way, on each GitHub release the package gets published on PyPi and the associated documentation is published on GitHub Pages.","title":"CI/CD"},{"location":"features/cicd/#cicd","text":"The generated project contains two GiHub Actions workflow to run the Test Suite and to publish you project. Note You need to enable the GitHub Actions from the settings in your repository. Important All the workflow already implement the logic needed to cache the conda and pip environment between workflow runs. Warning The annotated tags in the git repository to manage releases should follow the semantic versioning conventions: <major>.<minor>.<patch>","title":"CI/CD"},{"location":"features/cicd/#test-suite","text":"The Test Suite runs automatically for each commit in a Pull Request. It is successful if: The pre-commits do not raise any errors All the tests pass After that, the PR are marked with \u2714\ufe0f or \u274c depending on the test suite results.","title":"Test Suite"},{"location":"features/cicd/#publish-docs","text":"The first time you should use mike to: create the gh-pages branch and specify the default docs version. mike deploy 0 .1 latest --push mike set-default latest Warning You do not need to execute these commands if you accepted the optional cookiecutter setup step. Info Remember to enable the GitHub Pages from the repository settings. After that, the docs are built and automatically published on release on GitHub Pages. This means that every time you publish a new release in your project an associated version of the documentation is published. Important The documentation version utilizes only the <major>.<minor> version of the release tag, discarding the patch version.","title":"Publish docs"},{"location":"features/cicd/#publish-pypi","text":"To publish your package on PyPi it is enough to configure the PyPi token in the GitHub repository secrets and de-comment the following in the publish.yaml workflow: - name : Build SDist and wheel run : pipx run build - name : Check metadata run : pipx run twine check dist/* - name : Publish distribution \ud83d\udce6 to PyPI uses : pypa/gh-action-pypi-publish@release/v1 with : user : __token__ password : ${{ secrets.PYPI_API_TOKEN }} In this way, on each GitHub release the package gets published on PyPi and the associated documentation is published on GitHub Pages.","title":"Publish PyPi"},{"location":"features/conda/","text":"Python Environment \u00b6 The generated project is a Python Package, whose dependencies are defined in the setup.cfg The development setup comprises a conda environment which installs the package itself in edit mode. Dependencies \u00b6 All the project dependencies should be defined in the setup.cfg as pip dependencies. In rare cases, it is useful to specify conda dependencies --- they will not be resolved when installing the package from PyPi. This division is useful when installing particular or optimized packages such a PyTorch and PyTorch Geometric. Hint It is possible to manage the Python version to use in the conda env.yaml . Info This organization allows for conda and pip dependencies to co-exhist, which in practice happens a lot in research projects. Update \u00b6 In order to update the pip dependencies after changing the setup.cfg it is enough to run: pip install -e '.[dev]'","title":"Python Environment"},{"location":"features/conda/#python-environment","text":"The generated project is a Python Package, whose dependencies are defined in the setup.cfg The development setup comprises a conda environment which installs the package itself in edit mode.","title":"Python Environment"},{"location":"features/conda/#dependencies","text":"All the project dependencies should be defined in the setup.cfg as pip dependencies. In rare cases, it is useful to specify conda dependencies --- they will not be resolved when installing the package from PyPi. This division is useful when installing particular or optimized packages such a PyTorch and PyTorch Geometric. Hint It is possible to manage the Python version to use in the conda env.yaml . Info This organization allows for conda and pip dependencies to co-exhist, which in practice happens a lot in research projects.","title":"Dependencies"},{"location":"features/conda/#update","text":"In order to update the pip dependencies after changing the setup.cfg it is enough to run: pip install -e '.[dev]'","title":"Update"},{"location":"features/determinism/","text":"Determinism \u00b6 The template always logs the seed utilized in order to guarantee reproducibility . The user specifies a seed_index value in the configuration train/default.yaml : seed_index : 1 deterministic : False This value indexes an array of deterministic but randomly generated seeds, e.g.: Setting seed 1273642419 from seeds [ 1 ] Hint This setup allows to easily run the same experiment with different seeds in a reproducible way. It is enough to run a Hydra multi-run over the seed_index . The following would run the same experiment with five different seeds, which can be analyzed in the logger dashboard: python src/project/run.py -m train.seed_index = 1 ,2,3,4 Info The deterministic option deterministic: False controls the use of deterministic algorithms in PyTorch, it is forwarded to the Lightning Trainer.","title":"Determinism"},{"location":"features/determinism/#determinism","text":"The template always logs the seed utilized in order to guarantee reproducibility . The user specifies a seed_index value in the configuration train/default.yaml : seed_index : 1 deterministic : False This value indexes an array of deterministic but randomly generated seeds, e.g.: Setting seed 1273642419 from seeds [ 1 ] Hint This setup allows to easily run the same experiment with different seeds in a reproducible way. It is enough to run a Hydra multi-run over the seed_index . The following would run the same experiment with five different seeds, which can be analyzed in the logger dashboard: python src/project/run.py -m train.seed_index = 1 ,2,3,4 Info The deterministic option deterministic: False controls the use of deterministic algorithms in PyTorch, it is forwarded to the Lightning Trainer.","title":"Determinism"},{"location":"features/docs/","text":"Documentation \u00b6 MkDocs and Material for MkDocs is already configured in the generated project. In order to create your docs it is enough to: Modify the nav index in the mkdocs.yaml , which describes how to organize the pages. An example of the nav is the following: nav : - Home : index.md - Getting started : - Generating your project : getting-started/generation.md - Strucure : getting-started/structure.md Create all the files referenced in the nav relative to the docs/ folder. \u276f tree docs docs \u251c\u2500\u2500 getting-started \u2502 \u251c\u2500\u2500 generation.md \u2502 \u2514\u2500\u2500 structure.md \u2514\u2500\u2500 index.md To preview your documentation it is enough to run mkdocs serve . To manually deploy the documentation see mike , or see the integrated GitHub Action to publish the docs on release .","title":"Docs"},{"location":"features/docs/#documentation","text":"MkDocs and Material for MkDocs is already configured in the generated project. In order to create your docs it is enough to: Modify the nav index in the mkdocs.yaml , which describes how to organize the pages. An example of the nav is the following: nav : - Home : index.md - Getting started : - Generating your project : getting-started/generation.md - Strucure : getting-started/structure.md Create all the files referenced in the nav relative to the docs/ folder. \u276f tree docs docs \u251c\u2500\u2500 getting-started \u2502 \u251c\u2500\u2500 generation.md \u2502 \u2514\u2500\u2500 structure.md \u2514\u2500\u2500 index.md To preview your documentation it is enough to run mkdocs serve . To manually deploy the documentation see mike , or see the integrated GitHub Action to publish the docs on release .","title":"Documentation"},{"location":"features/envvars/","text":"Environment Variables \u00b6 System specific variables (e.g. absolute paths) should not be under version control, otherwise there will be conflicts between different users. The best way to handle system specific variables is through environment variables. You can define new environment variables in a .env file in the project root. A copy of this file (e.g. .env.template ) can be under version control to ease new project configurations. To define a new variable write inside .env : export MY_VAR = /home/user/my_system_path You can dynamically resolve the variable name from everywhere python yaml posix In Python code use: get_env ( \"MY_VAR\" ) In the Hydra yaml configurations: ${oc.env:MY_VAR} In posix shells: . .env echo $MY_VAR","title":"Environment variables"},{"location":"features/envvars/#environment-variables","text":"System specific variables (e.g. absolute paths) should not be under version control, otherwise there will be conflicts between different users. The best way to handle system specific variables is through environment variables. You can define new environment variables in a .env file in the project root. A copy of this file (e.g. .env.template ) can be under version control to ease new project configurations. To define a new variable write inside .env : export MY_VAR = /home/user/my_system_path You can dynamically resolve the variable name from everywhere python yaml posix In Python code use: get_env ( \"MY_VAR\" ) In the Hydra yaml configurations: ${oc.env:MY_VAR} In posix shells: . .env echo $MY_VAR","title":"Environment Variables"},{"location":"features/fastdevrun/","text":"Fast Dev Run \u00b6 The template expands the Lightning fast_dev_run mode to be more debugging friendly. It will also: Disable multiple workers in the dataloaders Use the CPU and not the GPU Info It is possible to modify this behaviour by simply modifying the run.py file.","title":"Fast dev run"},{"location":"features/fastdevrun/#fast-dev-run","text":"The template expands the Lightning fast_dev_run mode to be more debugging friendly. It will also: Disable multiple workers in the dataloaders Use the CPU and not the GPU Info It is possible to modify this behaviour by simply modifying the run.py file.","title":"Fast Dev Run"},{"location":"features/metadata/","text":"MetaData \u00b6 The bridge between the Lightning DataModule and the Lightning Module. It is responsible for collecting data information to be fed to the module. The Lightning Module will receive an instance of MetaData when instantiated, both in the train loop or when restored from a checkpoint. Warning MetaData exposes save and load . Those are two user-defined methods that specify how to serialize and de-serialize the information contained in its attributes. This is needed for the checkpointing restore to work properly and must be always implemented , where the metadata is needed. This decoupling allows the architecture to be parametric (e.g. in the number of classes) and DataModule/Trainer independent (useful in prediction scenarios). Examples are the class names in a classification task or the vocabulary in NLP tasks.","title":"Metadata"},{"location":"features/metadata/#metadata","text":"The bridge between the Lightning DataModule and the Lightning Module. It is responsible for collecting data information to be fed to the module. The Lightning Module will receive an instance of MetaData when instantiated, both in the train loop or when restored from a checkpoint. Warning MetaData exposes save and load . Those are two user-defined methods that specify how to serialize and de-serialize the information contained in its attributes. This is needed for the checkpointing restore to work properly and must be always implemented , where the metadata is needed. This decoupling allows the architecture to be parametric (e.g. in the number of classes) and DataModule/Trainer independent (useful in prediction scenarios). Examples are the class names in a classification task or the vocabulary in NLP tasks.","title":"MetaData"},{"location":"features/nncore/","text":"NN Template core \u00b6 Most of the logic is abstracted from the template into an accompanying library: nn-template-core . This library contains the logic necessary for the restore, logging, and many other functionalities implemented in the template. Info This decoupling eases the updating of the template, reaching a desirable tradeoff: template : easy to use and customize, hard to update library : hard to customize, easy to update With our approach updating most of the functions is extremely easy, it is just a Python dependency, while maintaing the flexibility of a template. Warning It is important to not remove the NNTemplateCore callback from the instantiated callbacks in the template. It is used to inject personalized behaviour in the training loop.","title":"Core"},{"location":"features/nncore/#nn-template-core","text":"Most of the logic is abstracted from the template into an accompanying library: nn-template-core . This library contains the logic necessary for the restore, logging, and many other functionalities implemented in the template. Info This decoupling eases the updating of the template, reaching a desirable tradeoff: template : easy to use and customize, hard to update library : hard to customize, easy to update With our approach updating most of the functions is extremely easy, it is just a Python dependency, while maintaing the flexibility of a template. Warning It is important to not remove the NNTemplateCore callback from the instantiated callbacks in the template. It is used to inject personalized behaviour in the training loop.","title":"NN Template core"},{"location":"features/restore/","text":"Restore \u00b6 The template offers a way to restore a previous run from the configuration. The relevant configuration block is in conf/train/default.yml : restore : ckpt_or_run_path : null mode : null # null, finetune, hotstart, continue ckpt_or_run_path \u00b6 The ckpt_or_run_path can be a path towards a Lightning Checkpoint or the run identifiers w.r.t. the logger. In case of W&B as a logger, they are called run_path and are in the form of entity/project/run_id . Warning If ckpt_or_run_path points to a checkpoint, that checkpoint must have been saved with this template, because additional information are attached to the checkpoint to guarantee a correct restore. These include the run_path itself and the whole configuration used. mode \u00b6 We support 4 different modes for restoring an experiment: null finetune hotstart continue restore : mode : null In this mode no restore happens, and ckpt_or_run_path is ignored. Use Case This is the default option and allows the user to train the model from scratch logging into a new run. restore : mode : finetune In this mode only the model weights are restored, both the Trainer state and the logger run are not restored . Use Case As the name suggest, one of the most common use case is when fine tuning a trained model logging into a new run with a novel training regimen. restore : mode : hotstart In this mode the training continues from the checkpoint restoring the Trainer state but the logging does not. A new run is created on the logger dashboard. Use Case Perform different tests in separate logging runs branching from the same trained model. restore : mode : continue In this mode the training continues from the checkpoint and the logging continues in the previous run. No new run is created on the logger dashboard. Use Case The training execution was interrupted and the user wants to continue it. Restore summary null finetune hotstart continue Model weights Trainer state Logging run","title":"Restore"},{"location":"features/restore/#restore","text":"The template offers a way to restore a previous run from the configuration. The relevant configuration block is in conf/train/default.yml : restore : ckpt_or_run_path : null mode : null # null, finetune, hotstart, continue","title":"Restore"},{"location":"features/restore/#ckpt_or_run_path","text":"The ckpt_or_run_path can be a path towards a Lightning Checkpoint or the run identifiers w.r.t. the logger. In case of W&B as a logger, they are called run_path and are in the form of entity/project/run_id . Warning If ckpt_or_run_path points to a checkpoint, that checkpoint must have been saved with this template, because additional information are attached to the checkpoint to guarantee a correct restore. These include the run_path itself and the whole configuration used.","title":"ckpt_or_run_path"},{"location":"features/restore/#mode","text":"We support 4 different modes for restoring an experiment: null finetune hotstart continue restore : mode : null In this mode no restore happens, and ckpt_or_run_path is ignored. Use Case This is the default option and allows the user to train the model from scratch logging into a new run. restore : mode : finetune In this mode only the model weights are restored, both the Trainer state and the logger run are not restored . Use Case As the name suggest, one of the most common use case is when fine tuning a trained model logging into a new run with a novel training regimen. restore : mode : hotstart In this mode the training continues from the checkpoint restoring the Trainer state but the logging does not. A new run is created on the logger dashboard. Use Case Perform different tests in separate logging runs branching from the same trained model. restore : mode : continue In this mode the training continues from the checkpoint and the logging continues in the previous run. No new run is created on the logger dashboard. Use Case The training execution was interrupted and the user wants to continue it. Restore summary null finetune hotstart continue Model weights Trainer state Logging run","title":"mode"},{"location":"features/storage/","text":"Storage \u00b6 The checkpoints and other data produces by the experiment is stored in a logger agnostic folder defined in the configuration core.storage_dir This is the organization of the storage_dir : storage \u2514\u2500\u2500 <project_name> \u2514\u2500\u2500 <run_id> \u251c\u2500\u2500 checkpoints \u2502 \u2514\u2500\u2500 <checkpoint_name>.ckpt.zip \u2514\u2500\u2500 config.yaml In the configuration it is possible to specify whether the run files stored inside the storage_dir should be uploaded to the cloud: logging : upload : run_files : true source : true","title":"Storage"},{"location":"features/storage/#storage","text":"The checkpoints and other data produces by the experiment is stored in a logger agnostic folder defined in the configuration core.storage_dir This is the organization of the storage_dir : storage \u2514\u2500\u2500 <project_name> \u2514\u2500\u2500 <run_id> \u251c\u2500\u2500 checkpoints \u2502 \u2514\u2500\u2500 <checkpoint_name>.ckpt.zip \u2514\u2500\u2500 config.yaml In the configuration it is possible to specify whether the run files stored inside the storage_dir should be uploaded to the cloud: logging : upload : run_files : true source : true","title":"Storage"},{"location":"features/tags/","text":"Tags \u00b6 Each run should be tagged in order to easily filter them from the logged dashboard. Unfortunately, it is easy to forget to tag correctly each run. We ask interactively for a list of comma separated tags, if those are not already defined in the configuration: WARNING No tags provided, asking for tags... Enter a list of comma separated tags (develop): Info If the current experiment is a sweep comprised of multiple runs and there are not any tags defined, an error is raised instead: ERROR You need to specify 'core.tags' in a multi-run setting!","title":"Tags"},{"location":"features/tags/#tags","text":"Each run should be tagged in order to easily filter them from the logged dashboard. Unfortunately, it is easy to forget to tag correctly each run. We ask interactively for a list of comma separated tags, if those are not already defined in the configuration: WARNING No tags provided, asking for tags... Enter a list of comma separated tags (develop): Info If the current experiment is a sweep comprised of multiple runs and there are not any tags defined, an error is raised instead: ERROR You need to specify 'core.tags' in a multi-run setting!","title":"Tags"},{"location":"features/tests/","text":"Tests \u00b6 The generated project includes automated tests that use the current configuration defined in your project. You should write additional tests specific to each project, but running the tests should give an idea at least if the code and fundamental operations work as expected. Info You can execute the tests with: pytest -v","title":"Tests"},{"location":"features/tests/#tests","text":"The generated project includes automated tests that use the current configuration defined in your project. You should write additional tests specific to each project, but running the tests should give an idea at least if the code and fundamental operations work as expected. Info You can execute the tests with: pytest -v","title":"Tests"},{"location":"getting-started/","text":"Principles behind nn-template \u00b6 When developing neural models ourselves, we often struggled with: Reproducibility . We strongly believe in the reproducibility requirement of scientific work. Framework Learning . Even when you find (or code yourself) the best framework to fit your needs, you still end up in messy situations when collaborating since others have to learn to use it; Avoiding boilerplate . We were bored to write the same code over and over in every project to handle the typical ML pipeline. Over the course of the years, we fine-tuned our toolbox to reach this local minimum with respect to our self-imposed requirements. After many epochs of training, the result is nn-template . nn-template is not a framework It does not aim to sidestep the need to write code. It does not constrain your workflow more than PyTorch Lightning does.","title":"Principles behind nn-template"},{"location":"getting-started/#principles-behind-nn-template","text":"When developing neural models ourselves, we often struggled with: Reproducibility . We strongly believe in the reproducibility requirement of scientific work. Framework Learning . Even when you find (or code yourself) the best framework to fit your needs, you still end up in messy situations when collaborating since others have to learn to use it; Avoiding boilerplate . We were bored to write the same code over and over in every project to handle the typical ML pipeline. Over the course of the years, we fine-tuned our toolbox to reach this local minimum with respect to our self-imposed requirements. After many epochs of training, the result is nn-template . nn-template is not a framework It does not aim to sidestep the need to write code. It does not constrain your workflow more than PyTorch Lightning does.","title":"Principles behind nn-template"},{"location":"getting-started/generation/","text":"Initial Setup \u00b6 Cookiecutter \u00b6 nn-template is, by definition, a template to generate projects. It's a robust starting point for your projects, something that lets you skip the initial boilerplate in configuring the environment, tests and such. Since it is a blueprint to build upon, it has no utility in being installed via pip or similar tools. Instead, we rely on cookiecutter to manage the setup stages and deliver to you a ready-to-run project. It is a general-purpose tool that enables users to add their water of choice (variable configurations) to their particular Cup-a-Soup (the template to be setup). Installing cookiecutter cookiecutter can be installed via pip in any Python-enabled environment (it won't be the same used by the project once instantiated). Our advice is to install cookiecutter as a system utility via pipx : pipx install cookiecutter Then, we need to tell cookiecutter which template to work on: cookiecutter https://github.com/grok-ai/nn-template.git It will clone the nn-template repository in the background, call its interactive setup, and build your project's folder according to the given parametrization. The parametrized setup will take care of: Set up the development of a Python package Initializing a clean Git repository and add the GitHub remote of choice Create a new Conda environment to execute your code in This extra step via cookiecutter is done to avoid a lot of manual parametrization, unavoidable when cloning a template repository from scratch. Trust us, it is totally worth the bother! Building Blocks \u00b6 The generated project already contains a minimal working example. You are free to modify anything you want except for a few essential and high-level things that keep everything working. (again, this is not a framework!) . In particular mantain: Any LightningLogger you may want to use wrapped in a NNLogger The NNTemplateCore Lightning callback Hint The template bootstraps the project with most of the needed boilerplate. The remaining components to implement for your project are the following: Implement data pipeline Dataset Pytorch Lightning DataModule Implement neural modules Model Pytorch Lightning Module FAQs \u00b6 What is The Answer to the Ultimate Question of Life, the Universe, and Everything? 42 Why are the logs badly formatted in PyCharm? This is due to the fact that we are using Rich to handle the logging, and Rich is not compatible with customized terminals. As its documentation says: \" PyCharm users will need to enable \u201cemulate terminal\u201d in output console option in run/debug configuration to see styled output. \" Why are file paths not interactive in the terminal's output? We would like to know, too. How can I exclude specific file paths from pre-commit checks (e.g. pydocstyle)? While we encourage everyone to keep best-practices and standards enforced via the pre-commit utility, we also take into account situations where you just copy/paste code from the Internet and fixing it would be tedious. In those cases, the file .pre-commit-config.yaml has you covered. Each hook can receive an additional property, namely exclude where you can specify single files or patterns to be excluded when running that hook. For example, if you want to exclude a file named ugly_but_working_code.py from an annoying hook annoying_hook (most likely pydocstyle ): - repo : https://github.com/slow_coding/annoying_hook.git hooks : - id : annoying_hook exclude : ugly_but_working_code.py Future Features \u00b6 Optuna support Support different loggers other than WandB","title":"Generating your project"},{"location":"getting-started/generation/#initial-setup","text":"","title":"Initial Setup"},{"location":"getting-started/generation/#cookiecutter","text":"nn-template is, by definition, a template to generate projects. It's a robust starting point for your projects, something that lets you skip the initial boilerplate in configuring the environment, tests and such. Since it is a blueprint to build upon, it has no utility in being installed via pip or similar tools. Instead, we rely on cookiecutter to manage the setup stages and deliver to you a ready-to-run project. It is a general-purpose tool that enables users to add their water of choice (variable configurations) to their particular Cup-a-Soup (the template to be setup). Installing cookiecutter cookiecutter can be installed via pip in any Python-enabled environment (it won't be the same used by the project once instantiated). Our advice is to install cookiecutter as a system utility via pipx : pipx install cookiecutter Then, we need to tell cookiecutter which template to work on: cookiecutter https://github.com/grok-ai/nn-template.git It will clone the nn-template repository in the background, call its interactive setup, and build your project's folder according to the given parametrization. The parametrized setup will take care of: Set up the development of a Python package Initializing a clean Git repository and add the GitHub remote of choice Create a new Conda environment to execute your code in This extra step via cookiecutter is done to avoid a lot of manual parametrization, unavoidable when cloning a template repository from scratch. Trust us, it is totally worth the bother!","title":"Cookiecutter"},{"location":"getting-started/generation/#building-blocks","text":"The generated project already contains a minimal working example. You are free to modify anything you want except for a few essential and high-level things that keep everything working. (again, this is not a framework!) . In particular mantain: Any LightningLogger you may want to use wrapped in a NNLogger The NNTemplateCore Lightning callback Hint The template bootstraps the project with most of the needed boilerplate. The remaining components to implement for your project are the following: Implement data pipeline Dataset Pytorch Lightning DataModule Implement neural modules Model Pytorch Lightning Module","title":"Building Blocks"},{"location":"getting-started/generation/#faqs","text":"What is The Answer to the Ultimate Question of Life, the Universe, and Everything? 42 Why are the logs badly formatted in PyCharm? This is due to the fact that we are using Rich to handle the logging, and Rich is not compatible with customized terminals. As its documentation says: \" PyCharm users will need to enable \u201cemulate terminal\u201d in output console option in run/debug configuration to see styled output. \" Why are file paths not interactive in the terminal's output? We would like to know, too. How can I exclude specific file paths from pre-commit checks (e.g. pydocstyle)? While we encourage everyone to keep best-practices and standards enforced via the pre-commit utility, we also take into account situations where you just copy/paste code from the Internet and fixing it would be tedious. In those cases, the file .pre-commit-config.yaml has you covered. Each hook can receive an additional property, namely exclude where you can specify single files or patterns to be excluded when running that hook. For example, if you want to exclude a file named ugly_but_working_code.py from an annoying hook annoying_hook (most likely pydocstyle ): - repo : https://github.com/slow_coding/annoying_hook.git hooks : - id : annoying_hook exclude : ugly_but_working_code.py","title":"FAQs"},{"location":"getting-started/generation/#future-features","text":"Optuna support Support different loggers other than WandB","title":"Future Features"},{"location":"integrations/dvc/","text":"Data Version Control \u00b6 DVC runs alongside git and uses the current commit hash to version control the data. Initialize the dvc repository: $ dvc init To start tracking a file or directory, use dvc add : $ dvc add data/ImageNet DVC stores information about the added file (or a directory) in a special .dvc file named data/ImageNet.dvc , a small text file with a human-readable format. This file can be easily versioned like source code with Git, as a placeholder for the original data (which gets listed in .gitignore ): git add data/ImageNet.dvc data/.gitignore git commit -m \"Add raw data\" Making changes \u00b6 When you make a change to a file or directory, run dvc add again to track the latest version: $ dvc add data/ImageNet Switching between versions \u00b6 The regular workflow is to use git checkout first to switch a branch, checkout a commit, or a revision of a .dvc file, and then run dvc checkout to sync data: $ git checkout <...> $ dvc checkout Info Read more in the DVC docs !","title":"DVC"},{"location":"integrations/dvc/#data-version-control","text":"DVC runs alongside git and uses the current commit hash to version control the data. Initialize the dvc repository: $ dvc init To start tracking a file or directory, use dvc add : $ dvc add data/ImageNet DVC stores information about the added file (or a directory) in a special .dvc file named data/ImageNet.dvc , a small text file with a human-readable format. This file can be easily versioned like source code with Git, as a placeholder for the original data (which gets listed in .gitignore ): git add data/ImageNet.dvc data/.gitignore git commit -m \"Add raw data\"","title":"Data Version Control"},{"location":"integrations/dvc/#making-changes","text":"When you make a change to a file or directory, run dvc add again to track the latest version: $ dvc add data/ImageNet","title":"Making changes"},{"location":"integrations/dvc/#switching-between-versions","text":"The regular workflow is to use git checkout first to switch a branch, checkout a commit, or a revision of a .dvc file, and then run dvc checkout to sync data: $ git checkout <...> $ dvc checkout Info Read more in the DVC docs !","title":"Switching between versions"},{"location":"integrations/githubactions/","text":"GitHub Actions \u00b6 Automate, customize, and execute your software development workflows right in your repository with GitHub Actions. Info The template offers workflows to automatically run tests and pre-commits on pull requests, publish on PyPi and the docs on GitHub Pages on release.","title":"GitHub Actions"},{"location":"integrations/githubactions/#github-actions","text":"Automate, customize, and execute your software development workflows right in your repository with GitHub Actions. Info The template offers workflows to automatically run tests and pre-commits on pull requests, publish on PyPi and the docs on GitHub Pages on release.","title":"GitHub Actions"},{"location":"integrations/hydra/","text":"Hydra \u00b6 Hydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads. The basic functionalities are intuitive: it is enough to change the configuration files in conf/* accordingly to your preferences. Everything will be logged in wandb automatically. Consider creating new root configurations conf/myawesomeexp.yaml instead of always using the default conf/default.yaml . Multi-run \u00b6 You can easily perform hyperparameters sweeps , which override the configuration defined in /conf/* . The easiest one is the grid-search. It executes the code with every possible combinations of the specified hyperparameters: python src/run.py -m optim.optimizer.lr = 0 .02,0.002,0.0002 optim.lr_scheduler.T_mult = 1 ,2 optim.optimizer.weight_decay = 0 ,1e-5 You can explore aggregate statistics or compare and analyze each run in the W&B dashboard. Info We recommend to go through at least the Basic Tutorial , keep in mind that Hydra builds on top of OmegaConf .","title":"Hydra"},{"location":"integrations/hydra/#hydra","text":"Hydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads. The basic functionalities are intuitive: it is enough to change the configuration files in conf/* accordingly to your preferences. Everything will be logged in wandb automatically. Consider creating new root configurations conf/myawesomeexp.yaml instead of always using the default conf/default.yaml .","title":"Hydra"},{"location":"integrations/hydra/#multi-run","text":"You can easily perform hyperparameters sweeps , which override the configuration defined in /conf/* . The easiest one is the grid-search. It executes the code with every possible combinations of the specified hyperparameters: python src/run.py -m optim.optimizer.lr = 0 .02,0.002,0.0002 optim.lr_scheduler.T_mult = 1 ,2 optim.optimizer.weight_decay = 0 ,1e-5 You can explore aggregate statistics or compare and analyze each run in the W&B dashboard. Info We recommend to go through at least the Basic Tutorial , keep in mind that Hydra builds on top of OmegaConf .","title":"Multi-run"},{"location":"integrations/lightning/","text":"PyTorch Lightning \u00b6 Lightning makes coding complex networks simple. It is not a high level framework like keras , but forces a neat code organization and encapsulation. You should be somewhat familiar with PyTorch and PyTorch Lightning before using this template.","title":"PyTorch Lightning"},{"location":"integrations/lightning/#pytorch-lightning","text":"Lightning makes coding complex networks simple. It is not a high level framework like keras , but forces a neat code organization and encapsulation. You should be somewhat familiar with PyTorch and PyTorch Lightning before using this template.","title":"PyTorch Lightning"},{"location":"integrations/mkdocs/","text":"MkDocs \u00b6 MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Material for MkDocs \u00b6 Material for MkDocs is a theme for MkDocs, a static site generator geared towards (technical) project documentation. Hint The template comes with Material for MkDocs already configured, to create your documentation you only need to write markdown files and define the nav . See the Documentation page to get started!","title":"MkDocs"},{"location":"integrations/mkdocs/#mkdocs","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file.","title":"MkDocs"},{"location":"integrations/mkdocs/#material-for-mkdocs","text":"Material for MkDocs is a theme for MkDocs, a static site generator geared towards (technical) project documentation. Hint The template comes with Material for MkDocs already configured, to create your documentation you only need to write markdown files and define the nav . See the Documentation page to get started!","title":"Material for MkDocs"},{"location":"integrations/streamlit/","text":"Streamlit \u00b6 Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes, you can build and deploy powerful data apps to: Explore your data Interact with your model Analyze your model behavior and input sensitivity Showcase your prototype with awesome web apps Moreover, Streamlit enables interactive development with automatic rerun on files changes. Info Launch a minimal app with PYTHONPATH=. streamlit run src/ui/run.py . There is a built-in function to restore a model checkpoint stored on W&B, with automatic download if the checkpoint is not present in the local machine:","title":"Streamlit"},{"location":"integrations/streamlit/#streamlit","text":"Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes, you can build and deploy powerful data apps to: Explore your data Interact with your model Analyze your model behavior and input sensitivity Showcase your prototype with awesome web apps Moreover, Streamlit enables interactive development with automatic rerun on files changes. Info Launch a minimal app with PYTHONPATH=. streamlit run src/ui/run.py . There is a built-in function to restore a model checkpoint stored on W&B, with automatic download if the checkpoint is not present in the local machine:","title":"Streamlit"},{"location":"integrations/wandb/","text":"Weights and Biases \u00b6 Weights & Biases helps you keep track of your machine learning projects. Use tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues. This is an example of a simple dashboard. Quickstart \u00b6 Login to your wandb account, running once wandb login . Configure the logging in conf/logging/* . Info Read more in the docs . Particularly useful the log method , accessible from inside a PyTorch Lightning module with self.logger.experiment.log .","title":"Weigth & Biases"},{"location":"integrations/wandb/#weights-and-biases","text":"Weights & Biases helps you keep track of your machine learning projects. Use tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues. This is an example of a simple dashboard.","title":"Weights and Biases"},{"location":"integrations/wandb/#quickstart","text":"Login to your wandb account, running once wandb login . Configure the logging in conf/logging/* . Info Read more in the docs . Particularly useful the log method , accessible from inside a PyTorch Lightning module with self.logger.experiment.log .","title":"Quickstart"},{"location":"project-structure/","text":"","title":"Index"},{"location":"project-structure/conf/","text":"","title":"Conf"},{"location":"project-structure/structure/","text":"Structure \u00b6 . \u251c\u2500\u2500 conf \u2502 \u251c\u2500\u2500 default.yaml \u2502 \u251c\u2500\u2500 hydra \u2502 \u2502 \u2514\u2500\u2500 default.yaml \u2502 \u251c\u2500\u2500 nn \u2502 \u2502 \u2514\u2500\u2500 default.yaml \u2502 \u2514\u2500\u2500 train \u2502 \u2514\u2500\u2500 default.yaml \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 .gitignore \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 index.md \u2502 \u2514\u2500\u2500 overrides \u2502 \u2514\u2500\u2500 main.html \u251c\u2500\u2500 .editorconfig \u251c\u2500\u2500 .env \u251c\u2500\u2500 .env.template \u251c\u2500\u2500 env.yaml \u251c\u2500\u2500 .flake8 \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 publish.yml \u2502 \u2514\u2500\u2500 test_suite.yml \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 .pre-commit-config.yaml \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 setup.cfg \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 awesome_project \u2502 \u251c\u2500\u2500 data \u2502 \u2502 \u251c\u2500\u2500 datamodule.py \u2502 \u2502 \u251c\u2500\u2500 dataset.py \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 modules \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 module.py \u2502 \u251c\u2500\u2500 pl_modules \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 pl_module.py \u2502 \u251c\u2500\u2500 run.py \u2502 \u2514\u2500\u2500 ui \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 run.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 conftest.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 test_checkpoint.py \u251c\u2500\u2500 test_configuration.py \u251c\u2500\u2500 test_nn_core_integration.py \u251c\u2500\u2500 test_resume.py \u251c\u2500\u2500 test_storage.py \u2514\u2500\u2500 test_training.py","title":"Structure"},{"location":"project-structure/structure/#structure","text":". \u251c\u2500\u2500 conf \u2502 \u251c\u2500\u2500 default.yaml \u2502 \u251c\u2500\u2500 hydra \u2502 \u2502 \u2514\u2500\u2500 default.yaml \u2502 \u251c\u2500\u2500 nn \u2502 \u2502 \u2514\u2500\u2500 default.yaml \u2502 \u2514\u2500\u2500 train \u2502 \u2514\u2500\u2500 default.yaml \u251c\u2500\u2500 data \u2502 \u2514\u2500\u2500 .gitignore \u251c\u2500\u2500 docs \u2502 \u251c\u2500\u2500 index.md \u2502 \u2514\u2500\u2500 overrides \u2502 \u2514\u2500\u2500 main.html \u251c\u2500\u2500 .editorconfig \u251c\u2500\u2500 .env \u251c\u2500\u2500 .env.template \u251c\u2500\u2500 env.yaml \u251c\u2500\u2500 .flake8 \u251c\u2500\u2500 .github \u2502 \u2514\u2500\u2500 workflows \u2502 \u251c\u2500\u2500 publish.yml \u2502 \u2514\u2500\u2500 test_suite.yml \u251c\u2500\u2500 .gitignore \u251c\u2500\u2500 LICENSE \u251c\u2500\u2500 mkdocs.yml \u251c\u2500\u2500 .pre-commit-config.yaml \u251c\u2500\u2500 pyproject.toml \u251c\u2500\u2500 README.md \u251c\u2500\u2500 setup.cfg \u251c\u2500\u2500 setup.py \u251c\u2500\u2500 src \u2502 \u2514\u2500\u2500 awesome_project \u2502 \u251c\u2500\u2500 data \u2502 \u2502 \u251c\u2500\u2500 datamodule.py \u2502 \u2502 \u251c\u2500\u2500 dataset.py \u2502 \u2502 \u2514\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u251c\u2500\u2500 modules \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 module.py \u2502 \u251c\u2500\u2500 pl_modules \u2502 \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2502 \u2514\u2500\u2500 pl_module.py \u2502 \u251c\u2500\u2500 run.py \u2502 \u2514\u2500\u2500 ui \u2502 \u251c\u2500\u2500 __init__.py \u2502 \u2514\u2500\u2500 run.py \u2514\u2500\u2500 tests \u251c\u2500\u2500 conftest.py \u251c\u2500\u2500 __init__.py \u251c\u2500\u2500 test_checkpoint.py \u251c\u2500\u2500 test_configuration.py \u251c\u2500\u2500 test_nn_core_integration.py \u251c\u2500\u2500 test_resume.py \u251c\u2500\u2500 test_storage.py \u2514\u2500\u2500 test_training.py","title":"Structure"}]}