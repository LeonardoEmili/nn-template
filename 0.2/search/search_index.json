{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"NN Template \u00b6 Generic template to bootstrap your PyTorch project. Click on and avoid writing boilerplate code for: PyTorch Lightning , lightweight PyTorch wrapper for high-performance AI research. Hydra , a framework for elegantly configuring complex applications. DVC , track large files, directories, or ML models. Think \"Git for data\". Weights and Biases , organize and analyze machine learning experiments. (educational account available) Streamlit , turns data scripts into shareable web apps in minutes. nn-template is opinionated so you don't have to be. If you use this template, please add to your README . Usage Examples \u00b6 Checkout the mwe branch to view a minimum working example on MNIST.","title":"Home"},{"location":"#nn-template","text":"Generic template to bootstrap your PyTorch project. Click on and avoid writing boilerplate code for: PyTorch Lightning , lightweight PyTorch wrapper for high-performance AI research. Hydra , a framework for elegantly configuring complex applications. DVC , track large files, directories, or ML models. Think \"Git for data\". Weights and Biases , organize and analyze machine learning experiments. (educational account available) Streamlit , turns data scripts into shareable web apps in minutes. nn-template is opinionated so you don't have to be. If you use this template, please add to your README .","title":"NN Template"},{"location":"#usage-examples","text":"Checkout the mwe branch to view a minimum working example on MNIST.","title":"Usage Examples"},{"location":"changelog/","text":"See the changelog in the releases page.","title":"Changelog"},{"location":"upgrade/","text":"The need for upgrading the template itself is lessened thanks to the nn-template-core library decoupling. Info Update the nn-template-core library changing the version constraint in the setup.cfg . However, you can use cruft to automate also the template updates!","title":"Upgrade"},{"location":"features/bestpractices/","text":"Tooling \u00b6 The template configures are the tooling necessary for a modern python project. These include: EditorConfig maintain consistent coding styles for multiple developers. Black the uncompromising code formatter. isort sort imports alphabetically, and automatically separated into sections and by type. flake8 check coding style (PEP8), programming errors and cyclomatic complexity. pydocstyle static analysis tool for checking compliance with Python docstring conventions. MyPy static type checker for Python. Coverage measure code coverage of Python programs. bandit security linter from PyCQA. pre-commit framework for managing and maintaining pre-commit hooks. Pre commits \u00b6 The pre-commits configuration is defined in .pre-commit-config.yaml , and includes the most important checks and auto-fix to perform. If one of the pre-commits fails, the commit is aborted avoiding distraction errors. Info The pre-commits are also run in the CI/CD as part of the Test Suite. This helps guaranteeing the all the contributors are respecting the code conventions in the pull requests.","title":"Tooling"},{"location":"features/bestpractices/#tooling","text":"The template configures are the tooling necessary for a modern python project. These include: EditorConfig maintain consistent coding styles for multiple developers. Black the uncompromising code formatter. isort sort imports alphabetically, and automatically separated into sections and by type. flake8 check coding style (PEP8), programming errors and cyclomatic complexity. pydocstyle static analysis tool for checking compliance with Python docstring conventions. MyPy static type checker for Python. Coverage measure code coverage of Python programs. bandit security linter from PyCQA. pre-commit framework for managing and maintaining pre-commit hooks.","title":"Tooling"},{"location":"features/bestpractices/#pre-commits","text":"The pre-commits configuration is defined in .pre-commit-config.yaml , and includes the most important checks and auto-fix to perform. If one of the pre-commits fails, the commit is aborted avoiding distraction errors. Info The pre-commits are also run in the CI/CD as part of the Test Suite. This helps guaranteeing the all the contributors are respecting the code conventions in the pull requests.","title":"Pre commits"},{"location":"features/cicd/","text":"CI/CD \u00b6 The generated project contains two GiHub Actions workflow to run the Test Suite and to publish you project. Note You need to enable the GitHub Actions from the settings in your repository. Important All the workflow already implement the logic needed to cache the conda and pip environment between workflow runs. Warning The annotated tags in the git repository to manage releases should follow the semantic versioning conventions: <major>.<minor>.<patch> Test Suite \u00b6 The Test Suite runs automatically for each commit in a Pull Request. It is successful if: The pre-commits do not raise any errors All the tests pass After that, the PR are marked with \u2714\ufe0f or \u274c depending on the test suite results. Publish docs \u00b6 The first time you should create the gh-pages branch, specify the default docs version with mike and enable the GitHub Pages form the repository settings: mike deploy 0 .1 latest --push mike set-default latest The docs are built and automatically published on release on GitHub Pages. This means that every time you publish a new release in your project an associated version of the documentation is published. Info The documentation version utilizes only the <major>.<minor> version of the release tag, discarding the patch version. Publish PyPi \u00b6 To publish your package on PyPi it is enough to configure the PyPi token in the GitHub repository secrets and de-comment the following in the publish.yaml workflow: - name : Build SDist and wheel run : pipx run build - name : Check metadata run : pipx run twine check dist/* - name : Publish distribution \ud83d\udce6 to PyPI uses : pypa/gh-action-pypi-publish@release/v1 with : user : __token__ password : ${{ secrets.PYPI_API_TOKEN }} In this way, on each GitHub release the package gets published on PyPi and the associated documentation is published on GitHub Pages.","title":"CI/CD"},{"location":"features/cicd/#cicd","text":"The generated project contains two GiHub Actions workflow to run the Test Suite and to publish you project. Note You need to enable the GitHub Actions from the settings in your repository. Important All the workflow already implement the logic needed to cache the conda and pip environment between workflow runs. Warning The annotated tags in the git repository to manage releases should follow the semantic versioning conventions: <major>.<minor>.<patch>","title":"CI/CD"},{"location":"features/cicd/#test-suite","text":"The Test Suite runs automatically for each commit in a Pull Request. It is successful if: The pre-commits do not raise any errors All the tests pass After that, the PR are marked with \u2714\ufe0f or \u274c depending on the test suite results.","title":"Test Suite"},{"location":"features/cicd/#publish-docs","text":"The first time you should create the gh-pages branch, specify the default docs version with mike and enable the GitHub Pages form the repository settings: mike deploy 0 .1 latest --push mike set-default latest The docs are built and automatically published on release on GitHub Pages. This means that every time you publish a new release in your project an associated version of the documentation is published. Info The documentation version utilizes only the <major>.<minor> version of the release tag, discarding the patch version.","title":"Publish docs"},{"location":"features/cicd/#publish-pypi","text":"To publish your package on PyPi it is enough to configure the PyPi token in the GitHub repository secrets and de-comment the following in the publish.yaml workflow: - name : Build SDist and wheel run : pipx run build - name : Check metadata run : pipx run twine check dist/* - name : Publish distribution \ud83d\udce6 to PyPI uses : pypa/gh-action-pypi-publish@release/v1 with : user : __token__ password : ${{ secrets.PYPI_API_TOKEN }} In this way, on each GitHub release the package gets published on PyPi and the associated documentation is published on GitHub Pages.","title":"Publish PyPi"},{"location":"features/conda/","text":"Python Environment \u00b6 The generated project is a Python Package, whose dependencies are defined in the setup.cfg The development setup comprises a conda environment which installs the package itself in edit mode. Dependencies \u00b6 All the project dependencies should be defined in the setup.cfg as pip dependencies. In rare cases, it is useful to specify conda dependencies --- they will not be resolved when installing the package from PyPi. This division is useful when installing particular or optimized packages such a PyTorch and PyTorch Geometric. Hint It is possible to manage the Python version to use in the conda env.yaml . Info This organization allows for conda and pip dependencies to co-exhist, which in practice happens a lot in research projects. Update \u00b6 In order to update the pip dependencies after changing the setup.cfg it is enough to run: pip install -e '.[dev]'","title":"Python Environment"},{"location":"features/conda/#python-environment","text":"The generated project is a Python Package, whose dependencies are defined in the setup.cfg The development setup comprises a conda environment which installs the package itself in edit mode.","title":"Python Environment"},{"location":"features/conda/#dependencies","text":"All the project dependencies should be defined in the setup.cfg as pip dependencies. In rare cases, it is useful to specify conda dependencies --- they will not be resolved when installing the package from PyPi. This division is useful when installing particular or optimized packages such a PyTorch and PyTorch Geometric. Hint It is possible to manage the Python version to use in the conda env.yaml . Info This organization allows for conda and pip dependencies to co-exhist, which in practice happens a lot in research projects.","title":"Dependencies"},{"location":"features/conda/#update","text":"In order to update the pip dependencies after changing the setup.cfg it is enough to run: pip install -e '.[dev]'","title":"Update"},{"location":"features/determinism/","text":"Determinism \u00b6 The template always logs the seed utilized in order to guarantee reproducibility. The user specifies a seed_index value in the configuration train/default.yaml : seed_index : 0 deterministic : False This value indexes an array of deterministic but randomly generated seeds, e.g.: Setting seed 1273642419 from seeds [ 1 ] Hint This setup allows to easily run the same experiment with different seeds in a reproducible way. It is enough to run a Hydra multi-run over the seed_index . The following would run the same experiment with five different seeds, which can be analyzed in the logger dashboard: python src/project/run.py -m train.seed_index = 0 ,1,2,3,4 Info The deterministic option deterministic: False controls the use of deterministic algorithms in PyTorch, it is forwarded to the Lightning Trainer.","title":"Determinism"},{"location":"features/determinism/#determinism","text":"The template always logs the seed utilized in order to guarantee reproducibility. The user specifies a seed_index value in the configuration train/default.yaml : seed_index : 0 deterministic : False This value indexes an array of deterministic but randomly generated seeds, e.g.: Setting seed 1273642419 from seeds [ 1 ] Hint This setup allows to easily run the same experiment with different seeds in a reproducible way. It is enough to run a Hydra multi-run over the seed_index . The following would run the same experiment with five different seeds, which can be analyzed in the logger dashboard: python src/project/run.py -m train.seed_index = 0 ,1,2,3,4 Info The deterministic option deterministic: False controls the use of deterministic algorithms in PyTorch, it is forwarded to the Lightning Trainer.","title":"Determinism"},{"location":"features/docs/","text":"Documentation \u00b6 MkDocs and Material for MkDocs is already configured in the generated project. In order to create your docs it is enough to: Modify the nav index in the mkdocs.yaml , which describes how to organize the pages. An example of the nav is the following: nav : - Home : index.md - Getting started : - Generating your project : getting-started/generation.md - Strucure : getting-started/structure.md Create all the files referenced in the nav relative to the docs/ folder. \u276f tree docs docs \u251c\u2500\u2500 getting-started \u2502 \u251c\u2500\u2500 generation.md \u2502 \u2514\u2500\u2500 structure.md \u2514\u2500\u2500 index.md To preview your documentation it is enough to run mkdocs serve . To manually deploy the documentation see mike , or see the integrated GitHub Action to publish the docs on release .","title":"Docs"},{"location":"features/docs/#documentation","text":"MkDocs and Material for MkDocs is already configured in the generated project. In order to create your docs it is enough to: Modify the nav index in the mkdocs.yaml , which describes how to organize the pages. An example of the nav is the following: nav : - Home : index.md - Getting started : - Generating your project : getting-started/generation.md - Strucure : getting-started/structure.md Create all the files referenced in the nav relative to the docs/ folder. \u276f tree docs docs \u251c\u2500\u2500 getting-started \u2502 \u251c\u2500\u2500 generation.md \u2502 \u2514\u2500\u2500 structure.md \u2514\u2500\u2500 index.md To preview your documentation it is enough to run mkdocs serve . To manually deploy the documentation see mike , or see the integrated GitHub Action to publish the docs on release .","title":"Documentation"},{"location":"features/envvars/","text":"Environment Variables \u00b6 System specific variables (e.g. absolute paths) should not be under version control, otherwise there will be conflicts between different users. The best way to handle system specific variables is through environment variables. You can define new environment variables in a .env file in the project root. A copy of this file (e.g. .env.template ) can be under version control to ease new project configurations. To define a new variable write inside .env : export MY_VAR = /home/user/my_system_path You can dynamically resolve the variable name from everywhere python yaml posix In Python code use: get_env ( \"MY_VAR\" ) In the Hydra yaml configurations: ${oc.env:MY_VAR} In posix shells: . .env echo $MY_VAR","title":"Environment variables"},{"location":"features/envvars/#environment-variables","text":"System specific variables (e.g. absolute paths) should not be under version control, otherwise there will be conflicts between different users. The best way to handle system specific variables is through environment variables. You can define new environment variables in a .env file in the project root. A copy of this file (e.g. .env.template ) can be under version control to ease new project configurations. To define a new variable write inside .env : export MY_VAR = /home/user/my_system_path You can dynamically resolve the variable name from everywhere python yaml posix In Python code use: get_env ( \"MY_VAR\" ) In the Hydra yaml configurations: ${oc.env:MY_VAR} In posix shells: . .env echo $MY_VAR","title":"Environment Variables"},{"location":"features/fastdevrun/","text":"Fast Dev Run \u00b6 The template expands the Lightning fast_dev_run mode to be more debugging friendly. It will also: Disable multiple workers in the dataloaders Use the CPU and not the GPU Info It is possible to modify this behaviour by simply modifying the run.py file.","title":"Fast dev run"},{"location":"features/fastdevrun/#fast-dev-run","text":"The template expands the Lightning fast_dev_run mode to be more debugging friendly. It will also: Disable multiple workers in the dataloaders Use the CPU and not the GPU Info It is possible to modify this behaviour by simply modifying the run.py file.","title":"Fast Dev Run"},{"location":"features/nncore/","text":"NN Template core \u00b6 Most of the logic is abstracted from the template into an accompanying library: nn-tempalte-core . This library contains the logic necessary for the restore, logging, and many other functionalities implemented in the template. Info This decoupling eases the updating of the template, reaching a desiderable tradeoff: template : easy to use and customize, hard to update library : hard to customize, easy to update With our approach updating most of the functions is extremely easy, it is just a python dependency, while maintaing the flexibility of a template. Warning It is important to not remove the NNTemplateCore callback from the instantiated callbacks in the template. It is used to inject personalized behaviour in the training loop.","title":"Core"},{"location":"features/nncore/#nn-template-core","text":"Most of the logic is abstracted from the template into an accompanying library: nn-tempalte-core . This library contains the logic necessary for the restore, logging, and many other functionalities implemented in the template. Info This decoupling eases the updating of the template, reaching a desiderable tradeoff: template : easy to use and customize, hard to update library : hard to customize, easy to update With our approach updating most of the functions is extremely easy, it is just a python dependency, while maintaing the flexibility of a template. Warning It is important to not remove the NNTemplateCore callback from the instantiated callbacks in the template. It is used to inject personalized behaviour in the training loop.","title":"NN Template core"},{"location":"features/restore/","text":"Restore \u00b6 The template offers a way to restore a previous run from the configuration. The relevant configuration block is in conf/train/default.yml : restore : ckpt_or_run_path : null mode : null # null, continue, hotstart ckpt_or_run_path \u00b6 The ckpt_or_run_path can be a path towards a Lightning Checkpoint or the run identifies. In case of W&B it is called run_path and are in the form of entity/project/run_id . Warning If ckpt_or_run_path points to a checkpoint, that checkpoint must have been saved with this template, because additional information are attached to the checkpoint to guarantee a correct restore. These include the run_path itself and the whole configuration used. mode \u00b6 We support three different modes for restoring an experiment: continue hotstart null restore : mode : continue In this mode the training continues from the checkpoint and the logging continues in the previous run. No new run is created on the logger dashboard. restore : mode : hotstart In this mode the training continues from the checkpoint but the logging does not. A new run is created on the logger dashboard. restore : mode : null In this mode no restore happens, and ckpt_or_run_path is ignored.","title":"Restore"},{"location":"features/restore/#restore","text":"The template offers a way to restore a previous run from the configuration. The relevant configuration block is in conf/train/default.yml : restore : ckpt_or_run_path : null mode : null # null, continue, hotstart","title":"Restore"},{"location":"features/restore/#ckpt_or_run_path","text":"The ckpt_or_run_path can be a path towards a Lightning Checkpoint or the run identifies. In case of W&B it is called run_path and are in the form of entity/project/run_id . Warning If ckpt_or_run_path points to a checkpoint, that checkpoint must have been saved with this template, because additional information are attached to the checkpoint to guarantee a correct restore. These include the run_path itself and the whole configuration used.","title":"ckpt_or_run_path"},{"location":"features/restore/#mode","text":"We support three different modes for restoring an experiment: continue hotstart null restore : mode : continue In this mode the training continues from the checkpoint and the logging continues in the previous run. No new run is created on the logger dashboard. restore : mode : hotstart In this mode the training continues from the checkpoint but the logging does not. A new run is created on the logger dashboard. restore : mode : null In this mode no restore happens, and ckpt_or_run_path is ignored.","title":"mode"},{"location":"features/storage/","text":"Storage \u00b6 The checkpoints and other data produces by the experiment is stored in a logger agnostic folder defined in the configuration core.storage_dir This is the organization of the storage_dir : storage \u2514\u2500\u2500 <project_name> \u2514\u2500\u2500 <run_id> \u251c\u2500\u2500 checkpoints \u2502 \u2514\u2500\u2500 <checkpoint_name>.ckpt \u2514\u2500\u2500 config.yaml In the configuration it is possible to specify whether the run files stored inside the storage_dir should be uploaded to the cloud: logging : upload : run_files : true","title":"Storage"},{"location":"features/storage/#storage","text":"The checkpoints and other data produces by the experiment is stored in a logger agnostic folder defined in the configuration core.storage_dir This is the organization of the storage_dir : storage \u2514\u2500\u2500 <project_name> \u2514\u2500\u2500 <run_id> \u251c\u2500\u2500 checkpoints \u2502 \u2514\u2500\u2500 <checkpoint_name>.ckpt \u2514\u2500\u2500 config.yaml In the configuration it is possible to specify whether the run files stored inside the storage_dir should be uploaded to the cloud: logging : upload : run_files : true","title":"Storage"},{"location":"features/tags/","text":"Tags \u00b6 Each run should be tagged in order to easily filter them from the logged dashboard. Unfortunately, it is easy to forget to tag correctly each run. We ask interactively for a list of comma separated tags, if those are not already defined in the configuration: WARNING No tags provided, asking for tags... Enter a list of comma separated tags (develop): Info If the current experiment is a sweep comprised of multiple runs and there are not any tags defined, an error is raised instead: ERROR You need to specify 'core.tags' in a multi-run setting!","title":"Tags"},{"location":"features/tags/#tags","text":"Each run should be tagged in order to easily filter them from the logged dashboard. Unfortunately, it is easy to forget to tag correctly each run. We ask interactively for a list of comma separated tags, if those are not already defined in the configuration: WARNING No tags provided, asking for tags... Enter a list of comma separated tags (develop): Info If the current experiment is a sweep comprised of multiple runs and there are not any tags defined, an error is raised instead: ERROR You need to specify 'core.tags' in a multi-run setting!","title":"Tags"},{"location":"features/tests/","text":"Tests \u00b6 The generated project includes automated tests that use the current configuration defined in your project. You should write additional tests specific to each project, but running the tests should give an idea at least if the code and fundamental operations work as expected. Info You can execute the tests with: pytest -v","title":"Tests"},{"location":"features/tests/#tests","text":"The generated project includes automated tests that use the current configuration defined in your project. You should write additional tests specific to each project, but running the tests should give an idea at least if the code and fundamental operations work as expected. Info You can execute the tests with: pytest -v","title":"Tests"},{"location":"getting-started/generation/","text":"","title":"Generating your project"},{"location":"getting-started/structure/","text":"Structure \u00b6 . \u251c\u2500\u2500 .cache \u251c\u2500\u2500 conf # hydra compositional config \u2502 \u251c\u2500\u2500 nn \u2502 \u251c\u2500\u2500 default.yaml # current experiment configuration \u2502 \u251c\u2500\u2500 hydra \u2502 \u2514\u2500\u2500 train \u251c\u2500\u2500 data # datasets \u251c\u2500\u2500 .env # system-specific env variables, e.g. PROJECT_ROOT \u251c\u2500\u2500 requirements.txt # basic requirements \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 common # common modules and utilities \u2502 \u251c\u2500\u2500 data # PyTorch Lightning datamodules and datasets \u2502 \u251c\u2500\u2500 modules # PyTorch Lightning modules \u2502 \u251c\u2500\u2500 run.py # entry point to run current conf \u2502 \u2514\u2500\u2500 ui # interactive streamlit apps \u2514\u2500\u2500 wandb # local experiments (auto-generated)","title":"Strucure"},{"location":"getting-started/structure/#structure","text":". \u251c\u2500\u2500 .cache \u251c\u2500\u2500 conf # hydra compositional config \u2502 \u251c\u2500\u2500 nn \u2502 \u251c\u2500\u2500 default.yaml # current experiment configuration \u2502 \u251c\u2500\u2500 hydra \u2502 \u2514\u2500\u2500 train \u251c\u2500\u2500 data # datasets \u251c\u2500\u2500 .env # system-specific env variables, e.g. PROJECT_ROOT \u251c\u2500\u2500 requirements.txt # basic requirements \u251c\u2500\u2500 src \u2502 \u251c\u2500\u2500 common # common modules and utilities \u2502 \u251c\u2500\u2500 data # PyTorch Lightning datamodules and datasets \u2502 \u251c\u2500\u2500 modules # PyTorch Lightning modules \u2502 \u251c\u2500\u2500 run.py # entry point to run current conf \u2502 \u2514\u2500\u2500 ui # interactive streamlit apps \u2514\u2500\u2500 wandb # local experiments (auto-generated)","title":"Structure"},{"location":"integrations/dvc/","text":"Data Version Control \u00b6 DVC runs alongside git and uses the current commit hash to version control the data. Initialize the dvc repository: $ dvc init To start tracking a file or directory, use dvc add : $ dvc add data/ImageNet DVC stores information about the added file (or a directory) in a special .dvc file named data/ImageNet.dvc , a small text file with a human-readable format. This file can be easily versioned like source code with Git, as a placeholder for the original data (which gets listed in .gitignore ): git add data/ImageNet.dvc data/.gitignore git commit -m \"Add raw data\" Making changes \u00b6 When you make a change to a file or directory, run dvc add again to track the latest version: $ dvc add data/ImageNet Switching between versions \u00b6 The regular workflow is to use git checkout first to switch a branch, checkout a commit, or a revision of a .dvc file, and then run dvc checkout to sync data: $ git checkout <...> $ dvc checkout Info Read more in the DVC docs !","title":"DVC"},{"location":"integrations/dvc/#data-version-control","text":"DVC runs alongside git and uses the current commit hash to version control the data. Initialize the dvc repository: $ dvc init To start tracking a file or directory, use dvc add : $ dvc add data/ImageNet DVC stores information about the added file (or a directory) in a special .dvc file named data/ImageNet.dvc , a small text file with a human-readable format. This file can be easily versioned like source code with Git, as a placeholder for the original data (which gets listed in .gitignore ): git add data/ImageNet.dvc data/.gitignore git commit -m \"Add raw data\"","title":"Data Version Control"},{"location":"integrations/dvc/#making-changes","text":"When you make a change to a file or directory, run dvc add again to track the latest version: $ dvc add data/ImageNet","title":"Making changes"},{"location":"integrations/dvc/#switching-between-versions","text":"The regular workflow is to use git checkout first to switch a branch, checkout a commit, or a revision of a .dvc file, and then run dvc checkout to sync data: $ git checkout <...> $ dvc checkout Info Read more in the DVC docs !","title":"Switching between versions"},{"location":"integrations/githubactions/","text":"GitHub Actions \u00b6 Automate, customize, and execute your software development workflows right in your repository with GitHub Actions. Info The template offers worfloes to autmatically run tests and pre-commits on pull requestes, publish on PyPi and the docs on GitHub Pages on release.","title":"GitHub Actions"},{"location":"integrations/githubactions/#github-actions","text":"Automate, customize, and execute your software development workflows right in your repository with GitHub Actions. Info The template offers worfloes to autmatically run tests and pre-commits on pull requestes, publish on PyPi and the docs on GitHub Pages on release.","title":"GitHub Actions"},{"location":"integrations/hydra/","text":"Hydra \u00b6 Hydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads. The basic functionalities are intuitive: it is enough to change the configuration files in conf/* accordingly to your preferences. Everything will be logged in wandb automatically. Consider creating new root configurations conf/myawesomeexp.yaml instead of always using the default conf/default.yaml . Sweeps \u00b6 You can easily perform hyperparameters sweeps , which override the configuration defined in /conf/* . The easiest one is the grid-search. It executes the code with every possible combinations of the specified hyperparameters: PYTHONPATH = . python src/run.py -m optim.optimizer.lr = 0 .02,0.002,0.0002 optim.lr_scheduler.T_mult = 1 ,2 optim.optimizer.weight_decay = 0 ,1e-5 You can explore aggregate statistics or compare and analyze each run in the W&B dashboard. Info We recommend to go through at least the Basic Tutorial , and the docs about Instantiating objects with Hydra .","title":"Hydra"},{"location":"integrations/hydra/#hydra","text":"Hydra is an open-source Python framework that simplifies the development of research and other complex applications. The key feature is the ability to dynamically create a hierarchical configuration by composition and override it through config files and the command line. The name Hydra comes from its ability to run multiple similar jobs - much like a Hydra with multiple heads. The basic functionalities are intuitive: it is enough to change the configuration files in conf/* accordingly to your preferences. Everything will be logged in wandb automatically. Consider creating new root configurations conf/myawesomeexp.yaml instead of always using the default conf/default.yaml .","title":"Hydra"},{"location":"integrations/hydra/#sweeps","text":"You can easily perform hyperparameters sweeps , which override the configuration defined in /conf/* . The easiest one is the grid-search. It executes the code with every possible combinations of the specified hyperparameters: PYTHONPATH = . python src/run.py -m optim.optimizer.lr = 0 .02,0.002,0.0002 optim.lr_scheduler.T_mult = 1 ,2 optim.optimizer.weight_decay = 0 ,1e-5 You can explore aggregate statistics or compare and analyze each run in the W&B dashboard. Info We recommend to go through at least the Basic Tutorial , and the docs about Instantiating objects with Hydra .","title":"Sweeps"},{"location":"integrations/lightning/","text":"PyTorch Lightning \u00b6 Lightning makes coding complex networks simple. It is not a high level framework like keras , but forces a neat code organization and encapsulation. You should be somewhat familiar with PyTorch and PyTorch Lightning before using this template.","title":"PyTorch Lightning"},{"location":"integrations/lightning/#pytorch-lightning","text":"Lightning makes coding complex networks simple. It is not a high level framework like keras , but forces a neat code organization and encapsulation. You should be somewhat familiar with PyTorch and PyTorch Lightning before using this template.","title":"PyTorch Lightning"},{"location":"integrations/mkdocs/","text":"MkDocs \u00b6 MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introductory tutorial , then check the User Guide for more information. Material for MkDocs \u00b6 Material for MkDocs is a theme for MkDocs, a static site generator geared towards (technical) project documentation. Info The template comes with Material for MkDocs already configured, to create your documentation you only need to write markdown files and define the nav .","title":"MkDocs"},{"location":"integrations/mkdocs/#mkdocs","text":"MkDocs is a fast, simple and downright gorgeous static site generator that's geared towards building project documentation. Documentation source files are written in Markdown, and configured with a single YAML configuration file. Start by reading the introductory tutorial , then check the User Guide for more information.","title":"MkDocs"},{"location":"integrations/mkdocs/#material-for-mkdocs","text":"Material for MkDocs is a theme for MkDocs, a static site generator geared towards (technical) project documentation. Info The template comes with Material for MkDocs already configured, to create your documentation you only need to write markdown files and define the nav .","title":"Material for MkDocs"},{"location":"integrations/streamlit/","text":"Streamlit \u00b6 Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes, you can build and deploy powerful data apps to: Explore your data Interact with your model Analyze your model behavior and input sensitivity Showcase your prototype with awesome web apps Moreover, Streamlit enables interactive development with automatic rerun on files changes. Info Launch a minimal app with PYTHONPATH=. streamlit run src/ui/run.py . There is a built-in function to restore a model checkpoint stored on W&B, with automatic download if the checkpoint is not present in the local machine:","title":"Streamlit"},{"location":"integrations/streamlit/#streamlit","text":"Streamlit is an open-source Python library that makes it easy to create and share beautiful, custom web apps for machine learning and data science. In just a few minutes, you can build and deploy powerful data apps to: Explore your data Interact with your model Analyze your model behavior and input sensitivity Showcase your prototype with awesome web apps Moreover, Streamlit enables interactive development with automatic rerun on files changes. Info Launch a minimal app with PYTHONPATH=. streamlit run src/ui/run.py . There is a built-in function to restore a model checkpoint stored on W&B, with automatic download if the checkpoint is not present in the local machine:","title":"Streamlit"},{"location":"integrations/wandb/","text":"Weights and Biases \u00b6 Weights & Biases helps you keep track of your machine learning projects. Use tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues. This is an example of a simple dashboard. Quickstart \u00b6 Login to your wandb account, running once wandb login . Configure the logging in conf/logging/* . Info Read more in the docs . Particularly useful the log method , accessible from inside a PyTorch Lightning module with self.logger.experiment.log .","title":"Weigth & Biases"},{"location":"integrations/wandb/#weights-and-biases","text":"Weights & Biases helps you keep track of your machine learning projects. Use tools to log hyperparameters and output metrics from your runs, then visualize and compare results and quickly share findings with your colleagues. This is an example of a simple dashboard.","title":"Weights and Biases"},{"location":"integrations/wandb/#quickstart","text":"Login to your wandb account, running once wandb login . Configure the logging in conf/logging/* . Info Read more in the docs . Particularly useful the log method , accessible from inside a PyTorch Lightning module with self.logger.experiment.log .","title":"Quickstart"}]}